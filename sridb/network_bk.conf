netconfig = start
layer[0->1] = conv
  init_random = gaussian
  padding = 2
  stride = 4
  num_channels = 96
  num_groups = 1
  kernel_size = 5
layer[1->2] = relu
layer[2->3] = max_pooling
  padding = 0
  stride = 2
  kernel_size = 3
layer[3->4] = conv
  init_random = gaussian
  padding = 2
  stride = 1
  num_channels = 128
  num_groups = 1
  kernel_size = 3
layer[4->5] = relu
layer[5->6] = conv
  init_random = gaussian
  padding = 2
  stride = 1
  num_channels = 128
  num_groups = 1
  kernel_size = 3
layer[6->7] = relu
layer[7->8] = conv
  init_random = gaussian
  padding = 1
  stride = 1
  num_channels = 128
  num_groups = 1
  kernel_size = 3
layer[8->9] = relu
layer[9->10] = conv
  init_random = gaussian
  padding = 1
  stride = 1
  num_channels = 128
  num_groups = 1
  kernel_size = 3
layer[10->11] = relu
layer[11->12] = max_pooling
  padding = 0
  stride = 2
  kernel_size = 3
layer[12->13] = flatten
layer[13->14] = fullc
  init_sigma = 0.01
  init_random = gaussian
  init_bias = 0
  num_hidden_units = 512
layer[14->15] = relu
layer[15->16] = dropout
  threshold = 0.5
layer[16->17] = fullc
  init_sigma = 0.01
  init_random = gaussian
  init_bias = 0
  num_hidden_units = 512
layer[17->18] = relu
layer[18->19] = dropout
  threshold = 0.5
layer[19->20] = fullc
  init_sigma = 0.01
  init_random = gaussian
  init_bias = 0
  num_hidden_units = 121
layer[20->21] = softmax
netconfig = end

## network parameters
init_random = xavier
learning_rate = 0.01
input_shape = 3,48,48
batch_size = 256
bias_learning_rate = 0.02
learning_rate_gamma = 0.1
init_sigma = 0.01
bias_l2_regularization = 0.0
learning_rate_step = 10000
learning_rate_schedule = exponential_decay
l2_regularization = 0.0005
momentum = 0.9
## end network parameters
